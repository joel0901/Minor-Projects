
# For plotting 
import matplotlib.pyplot as plt

from Perceptron_BiPolarSigmoid import Perceptron

training_data = [[[0.6455696202531644,
   0.7954545454545454,
   0.20289855072463767,
   0.08000000000000002],
  0],
 [[0.5949367088607594,
   0.7272727272727273,
   0.18840579710144928,
   0.08000000000000002],
  0],
 [[0.5822784810126581,
   0.7045454545454546,
   0.2173913043478261,
   0.08000000000000002],
  0],
 [[0.6329113924050632,
   0.8181818181818181,
   0.20289855072463767,
   0.08000000000000002],
  0],
 [[0.6835443037974683,
   0.8863636363636364,
   0.2463768115942029,
   0.16000000000000003],
  0],
 [[0.6329113924050632,
   0.7727272727272727,
   0.2173913043478261,
   0.08000000000000002],
  0],
 [[0.5569620253164557,
   0.6590909090909091,
   0.20289855072463767,
   0.08000000000000002],
  0],
 [[0.620253164556962,
   0.7045454545454546,
   0.2173913043478261,
   0.04000000000000001],
  0],
 [[0.6075949367088607,
   0.7727272727272727,
   0.23188405797101452,
   0.08000000000000002],
  0],
 [[0.6075949367088607,
   0.6818181818181818,
   0.20289855072463767,
   0.04000000000000001],
  0],
 [[0.5443037974683543,
   0.6818181818181818,
   0.15942028985507248,
   0.04000000000000001],
  0],
 [[0.7341772151898733,
   0.9090909090909091,
   0.17391304347826086,
   0.08000000000000002],
  0],
 
 [[0.6835443037974683,
   0.8863636363636364,
   0.18840579710144928,
   0.16000000000000003],
  0],
 [[0.6455696202531644, 0.7954545454545454, 0.20289855072463767, 0.12], 0],
 [[0.7215189873417721, 0.8636363636363635, 0.2463768115942029, 0.12], 0],
 
 [[0.6835443037974683,
   0.7727272727272727,
   0.2463768115942029,
   0.08000000000000002],
  0],
 [[0.6455696202531644,
   0.8409090909090909,
   0.2173913043478261,
   0.16000000000000003],
  0],
 
 [[0.6455696202531644, 0.7499999999999999, 0.2463768115942029, 0.2], 0],
 [[0.6075949367088607,
   0.7727272727272727,
   0.2753623188405797,
   0.08000000000000002],
  0],
 [[0.6329113924050632,
   0.6818181818181818,
   0.23188405797101452,
   0.08000000000000002],
  0],
 [[0.6329113924050632,
   0.7727272727272727,
   0.23188405797101452,
   0.16000000000000003],
  0],
 [[0.6582278481012658,
   0.7727272727272727,
   0.20289855072463767,
   0.08000000000000002],
  0],
 [[0.5949367088607594,
   0.7272727272727273,
   0.23188405797101452,
   0.08000000000000002],
  0],
 [[0.6835443037974683,
   0.7727272727272727,
   0.2173913043478261,
   0.16000000000000003],
  0],
 [[0.6582278481012658,
   0.9318181818181817,
   0.2173913043478261,
   0.04000000000000001],
  0],
 [[0.6962025316455696,
   0.9545454545454546,
   0.20289855072463767,
   0.08000000000000002],
  0],
 [[0.620253164556962,
   0.7045454545454546,
   0.2173913043478261,
   0.08000000000000002],
  0],
 [[0.6329113924050632,
   0.7272727272727273,
   0.17391304347826086,
   0.08000000000000002],
  0],
 [[0.6962025316455696,
   0.7954545454545454,
   0.18840579710144928,
   0.08000000000000002],
  0],
 [[0.620253164556962,
   0.8181818181818181,
   0.20289855072463767,
   0.04000000000000001],
  0],
 [[0.5569620253164557,
   0.6818181818181818,
   0.18840579710144928,
   0.08000000000000002],
  0],
 [[0.6455696202531644,
   0.7727272727272727,
   0.2173913043478261,
   0.08000000000000002],
  0],
 [[0.6329113924050632, 0.7954545454545454, 0.18840579710144928, 0.12], 0],
 [[0.5696202531645569, 0.5227272727272727, 0.18840579710144928, 0.12], 0],
 [[0.6329113924050632, 0.7954545454545454, 0.23188405797101452, 0.24], 0],
 [[0.6455696202531644,
   0.8636363636363635,
   0.2753623188405797,
   0.16000000000000003],
  0],
 [[0.6075949367088607, 0.6818181818181818, 0.20289855072463767, 0.12], 0],
 [[0.5822784810126581,
   0.7272727272727273,
   0.20289855072463767,
   0.08000000000000002],
  0],
 [[0.670886075949367,
   0.8409090909090909,
   0.2173913043478261,
   0.08000000000000002],
  0],
 [[0.6329113924050632,
   0.7499999999999999,
   0.20289855072463767,
   0.08000000000000002],
  0],
 [[0.8860759493670884,
   0.7272727272727273,
   0.6811594202898551,
   0.5599999999999999],
  1],
 [[0.8101265822784809,
   0.7272727272727273,
   0.6521739130434783,
   0.6000000000000001],
  1],
 [[0.6962025316455696, 0.5227272727272727, 0.5797101449275363, 0.52], 1],
 [[0.8227848101265821,
   0.6363636363636364,
   0.6666666666666666,
   0.6000000000000001],
  1],
 [[0.7215189873417721, 0.6363636363636364, 0.6521739130434783, 0.52], 1],
 [[0.7974683544303796,
   0.7499999999999999,
   0.6811594202898551,
   0.6400000000000001],
  1],
 [[0.620253164556962, 0.5454545454545454, 0.4782608695652174, 0.4], 1],
 [[0.6582278481012658,
   0.6136363636363636,
   0.5652173913043478,
   0.5599999999999999],
  1],
 [[0.6329113924050632, 0.45454545454545453, 0.5072463768115942, 0.4], 1],
 [[0.7468354430379747,
   0.6818181818181818,
   0.6086956521739131,
   0.6000000000000001],
  1],
 [[0.7594936708860758, 0.5, 0.5797101449275363, 0.4], 1],
 [[0.772151898734177,
   0.6590909090909091,
   0.6811594202898551,
   0.5599999999999999],
  1],
 [[0.7088607594936708, 0.6590909090909091, 0.5217391304347826, 0.52], 1],
 [[0.8481012658227847,
   0.7045454545454546,
   0.6376811594202899,
   0.5599999999999999],
  1],
 [[0.7341772151898733, 0.6136363636363636, 0.5942028985507246, 0.4], 1],
 [[0.7088607594936708,
   0.5681818181818181,
   0.5652173913043478,
   0.44000000000000006],
  1],
 [[0.7468354430379747,
   0.7272727272727273,
   0.6956521739130435,
   0.7200000000000001],
  1],
 [[0.7974683544303796,
   0.5681818181818181,
   0.7101449275362319,
   0.6000000000000001],
  1],
 [[0.772151898734177, 0.6363636363636364, 0.6811594202898551, 0.48], 1],
 [[0.8101265822784809, 0.6590909090909091, 0.6231884057971014, 0.52], 1],
 [[0.8354430379746833,
   0.6818181818181818,
   0.6376811594202899,
   0.5599999999999999],
  1],
 [[0.8607594936708859,
   0.6363636363636364,
   0.6956521739130435,
   0.5599999999999999],
  1],
 [[0.8481012658227847, 0.6818181818181818, 0.7246376811594203, 0.68], 1],
 [[0.7215189873417721, 0.5909090909090909, 0.5072463768115942, 0.4], 1],
 [[0.6962025316455696,
   0.5454545454545454,
   0.5507246376811594,
   0.44000000000000006],
  1],
 [[0.6962025316455696, 0.5454545454545454, 0.5362318840579711, 0.4], 1],
 [[0.7341772151898733, 0.6136363636363636, 0.5652173913043478, 0.48], 1],
 [[0.7594936708860758,
   0.6136363636363636,
   0.7391304347826086,
   0.6400000000000001],
  1],
 [[0.6835443037974683,
   0.6818181818181818,
   0.6521739130434783,
   0.6000000000000001],
  1],

 [[0.8481012658227847,
   0.7045454545454546,
   0.6811594202898551,
   0.6000000000000001],
  1],
 [[0.7974683544303796, 0.5227272727272727, 0.6376811594202899, 0.52], 1],
 [[0.7088607594936708, 0.6818181818181818, 0.5942028985507246, 0.52], 1],
 [[0.6962025316455696, 0.5681818181818181, 0.5797101449275363, 0.52], 1],
 [[0.772151898734177,
   0.6818181818181818,
   0.6666666666666666,
   0.5599999999999999],
  1],
 [[0.7341772151898733, 0.5909090909090909, 0.5797101449275363, 0.48], 1],
 [[0.6329113924050632, 0.5227272727272727, 0.4782608695652174, 0.4], 1],
 [[0.7088607594936708, 0.6136363636363636, 0.6086956521739131, 0.52], 1],
 [[0.7215189873417721, 0.6818181818181818, 0.6086956521739131, 0.48], 1],
 [[0.7215189873417721, 0.6590909090909091, 0.6086956521739131, 0.52], 1],
 [[0.7215189873417721, 0.6363636363636364, 0.5942028985507246, 0.52], 1]]
 
# Create the perceptron
p = Perceptron(len(training_data[0][0]))

# Number of full iterations
epochs = 0
X = []
Y = []

while (epochs < 1500):

  # Epoch cumulative error
  error = 0
  square_error = 0

  # For each set in the training_data
  for value in training_data:

      # Calculate the result
      output = p.result(value[0])

      # Calculate the error
      iter_error = value[1] - output

      # Add the error to the epoch error
      error += iter_error

      # To calculate the Mean Square Error
      square_error += error * error

      # Adjust the weights based on inputs and the error
      p.weight_adjustment(value[0], iter_error)

  # Calculate the MSE - epoch error / number of sets
  mse = float(square_error/(2*len(training_data)))
  
  X.append(epochs)
  Y.append(mse)

  # Print the MSE for each epoch
  print("The mean squared error of %d epochs is %.10f" % (epochs, mse))

  # Every 100 epochs show the weight values
  if epochs % 100 == 0:
      print("0: %.10f - 1: %.10f - 2: %.10f - 3: %.10f" % (p.w[0], p.w[1], p.w[2], p.w[3]))

  # Increment the epoch number
  epochs += 1
  
print(X)
print(Y)

plt.plot(X, Y, 'bo')
plt.xlabel('Epochs')
plt.ylabel('MSE')
plt.title('Epoch vs MSE')
plt.show()


test_data = [[[0.620253164556962,
   0.6818181818181818,
   0.20289855072463767,
   0.08000000000000002],
  0],
 [[0.5822784810126581, 0.7727272727272727, 0.20289855072463767, 0.12], 0],
[[0.6835443037974683,
   0.8409090909090909,
   0.2173913043478261,
   0.08000000000000002],
  0],
[[0.7215189873417721, 1.0, 0.2173913043478261, 0.16000000000000003], 0],
[[0.6455696202531644, 0.8636363636363635, 0.2173913043478261, 0.12], 0],
[[0.5822784810126581,
   0.8181818181818181,
   0.14492753623188406,
   0.08000000000000002],
  0],
[[0.6582278481012658,
   0.7954545454545454,
   0.2173913043478261,
   0.08000000000000002],
  0],
[[0.6075949367088607,
   0.7045454545454546,
   0.23188405797101452,
   0.08000000000000002],
  0],
[[0.6455696202531644,
   0.8636363636363635,
   0.23188405797101452,
   0.08000000000000002],
  0],
[[0.5569620253164557,
   0.7272727272727273,
   0.18840579710144928,
   0.08000000000000002],
  0],
[[0.8734177215189872,
   0.7045454545454546,
   0.7101449275362319,
   0.6000000000000001],
  1],
 [[0.8354430379746833, 0.6590909090909091, 0.6666666666666666, 0.52], 1],
[[0.7088607594936708,
   0.6818181818181818,
   0.6521739130434783,
   0.6000000000000001],
  1],
[[0.772151898734177, 0.6363636363636364, 0.5797101449275363, 0.52], 1],
[[0.7594936708860758,
   0.6590909090909091,
   0.6521739130434783,
   0.6000000000000001],
  1],
 [[0.7594936708860758,
   0.7727272727272727,
   0.6521739130434783,
   0.6400000000000001],
  1],
[[0.6962025316455696, 0.5909090909090909, 0.6376811594202899, 0.48], 1],
[[0.7848101265822783, 0.6590909090909091, 0.6231884057971014, 0.52], 1],
 [[0.6455696202531644,
   0.5681818181818181,
   0.4347826086956522,
   0.44000000000000006],
  1],
 [[0.7848101265822783, 0.5, 0.6521739130434783, 0.6000000000000001], 1]]

test_error = 0

for value in test_data:
  det_cls = p.recall(value[0])
  if (det_cls != value[1]) : 
    test_error +=1
  print(det_cls)

print("The testing accuracy is\n")
print(1-test_error/len(test_data))